---
title: "Predicting costs"
author: ""
date: "10/20/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
---

\pagenumbering{arabic}

#1. Overview
The agency estimates project costs internally. Model variation in bids using a training data set (2015 through Q1 2019) and test it on a holdout set (Q2 and Q3). Use OLS and a penalized alternative. 

Project-level data includes a number of characteristics. Economic and demographic variables are specific to national and regional economic and labor market conditions. City of New York building permits can help proxy for activity in the broader regional construction market. ENR's cost index serves as proxy for broader hard construction costs. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
rm(list = ls()) # clear global environment 
cat("\014") # clear the console 
options(warn=-1)

# A number of programming tools commonly used for this analysis are loaded below; not all will be used - some are relics from early iterations of this work.

library(olsrr) 
library(data.table) 
library(ggplot2) 
library(lubridate) 
library(doBy) 
library(DataCombine) 
library(dplyr) 
library(compare) 
library(StatMeasures) 
library(caret) 
library(mlbench) 
library(glmnet) 
library(tidyr) 
library(tidyverse) 
library(broom) 
library(tidytext) 
library(widyr) 
library(Matrix) 
library(ggrepel) 
library(MLmetrics) 
library(mltools) 
```

```{r, echo=FALSE, include=TRUE}
#![Here's what we have in mind.](Estimator_Data.png)
```

The goal is to bridge exploration of the agency's data with potential exogenous predictors, some of which the internal estimation process may underestimate or inadvertently miss. If those predictors can add measurable value to the agency's cost estimation methods it may inform potential changes in methodology. 


#2. Data. 
Project data, called "bids" here, come from the Engineering Department. Economic and demographic indicators are specific to Greater New York (18 counties on both sides of the Hudson River) and come from the Planning and Regional Development Department; underlying data is from Oxford Economics.

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
setwd("~/Dropbox/Work and research/Port Authority/construction_bids") 
bids = read.csv("./Bid data 2019q3.csv") 
bids$Year = as.factor(bids$Year) 

bids$Date = as.Date(bids$Date,"%m/%d/%Y") 
bids = bids[order(bids$Date),] # Sort things by date 
bids$Engr.Est = as.numeric(bids$Engr.Est) 

bids$Quarter = quarter(bids$Date) 
bids$Quarter = paste("Q",quarter(bids$Date), sep="") 
bids$Q = paste(bids$Year,bids$Quarter,sep="-") 
```

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
econ = read.csv("./Economics 2019Q2.csv") 
econ$Q = paste(econ$Year,econ$Quarter,sep="-")

econ$Year = NULL  
econ$Quarter = NULL 
bids$Year = NULL 
bids$Quarter = NULL 

bids = merge(bids,econ, by = "Q", all.x=TRUE) 
rm(econ) 
```

Construction data is better from some parts of the region than others, but Jersey City's construction data is not yet as dependable as the City of New York's. NYC dominates regional construction anyway and it's justifiable for now to use its permitting data as a representative for the broader regional construction market. 

Prices of construction materials and labor already figure directly into the agency's internal cost estimation, and this analysis borrows the same index for predictive powers now despite uncertainty regarding whether its implicit presence in agency estimates helps or hurts its role in any multivariable considerations.  It is likely of second-order importance for now. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
permits_short = read.csv("./Permits_short.csv") 
permits_short$X=NULL 
names(permits_short) = c("Q","permits","permits_1") 

bids = merge(bids, permits_short, by = "Q", all.x=TRUE) 
rm(permits_short) 
``` 

```{r, echo=FALSE, include=FALSE}
cci = read.csv("./cci.csv") 
names(cci) = tolower(names(cci)) 
cci$avg. = NULL 
cci = gather(cci, month, cci, jan:dec, factor_key=TRUE) 
cci$month = gsub("(^[[:alpha:]])", "\\U\\1", cci$month, perl=TRUE)
cci$month = as.Date(paste(cci$month,"01",cci$year, sep="-"), format="%b-%d-%Y") 
cci = cci[month(cci$month) == 2 | month(cci$month) == 5 | month(cci$month) == 8 | month(cci$month) == 11, ] 

cci$Quarter = paste("Q",quarter(cci$month), sep="") 
cci$Q = paste(year(cci$month),cci$Quarter,sep="-") 
cci$Quarter = NULL 
cci$month = NULL 
cci$year=NULL 

bids = merge(bids, cci, by = "Q", all.x=TRUE) 

rm(cci) 
```

One variable of interest is the bidding process. Institutional discussions and earlier modeling suggests the bidding process may influence the bids. Limits placed on the range of bidders, for example, could, on average and holding other things constant, increase the average (and lowest qualifying) bid - this is basic microeconomics. I'll simplify the bidding format variable by making it binary: "public" for projects without significant constraints and "other" for ones, such as projects closed to firms not deemed "small business enterprises," that aren't. First I'll clean it a bit to consolidate near-dupliate categories. 

```{r echo=FALSE, include=FALSE}
bids$Format = as.factor(ifelse(bids$Format=="Public","Public", ifelse(bids$Format=="D-B"|bids$Format=="M/WBE"|bids$Format=="MBE"|bids$Format=="MBE/WBE"|bids$Format=="MWBE", "M/WBE",ifelse(bids$Format=="SBE","SBE","Other"))))
bids$Format2 = as.factor(ifelse(bids$Format=="Public","Public","Other")) 
```

Earlier work suggests there isn't major causal variation across the individual developing the in-house estimate and unique identifiers are omitted from this review. 

Agency projects last for months or years and actual costs do not exist for many of the observations, which at just over 260 projects already creates minor dimensionality concerns given the number of covariates. The second-lowest qualifying bid provides a reasonable target for evaluating internal estimates. The accuracy metric referenced through the exploratory discussion below and appendixed plots represents a ratio of that second-lowest bid over the estimate, both in dollars. A 1 would represent a case where the internal estimate (denominator) precisely matched the second-lowest bid; a 0.94 would mean the bidder bid 94 cents for every dollar estimated internally, et cetera. 

```{r, echo=TRUE, include=FALSE}
bids$accuracy = bids$Second.Bid / bids$Engr.Est 
```

```{r, echo=FALSE}
# Project size can be considered categorically. 
bids = bids %>%
  mutate(quantile = ntile(as.numeric(Second.Bid), 10))
bids$quantile = as.factor(bids$quantile) 
bids$quantile = ordered(bids$quantile, levels = 1:10) 
```


```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Reformat some of the variables, namely the economic indicators, to accommodate programming platform.  
bids$Employment.in.construction = as.numeric(as.character(bids$Employment.in.construction)) 
bids$Output.in.construction = as.numeric(as.character(bids$Output.in.construction))
bids$Total.population = as.numeric(as.character(bids$Total.population)) 
bids$Total.employment = as.numeric(as.character(bids$Total.employment)) 
bids$Total.output = as.numeric(as.character(bids$Total.output)) 
```

```{r, echo=TRUE}
bid = bids # backup my data frame 
```


#3. Exploratory analysis. 

Why might developing a conrolled multivariate model will be worth it? The average gap between bids and estimates is less than $900,000, or around 5% - the average project bid was $15 million. 

```{r, echo=FALSE, include=FALSE}
weighted.mean((bids$Second.Bid/bids$Engr.Est), bids$Engr.Est, na.rm = TRUE)
sum(bids$Second.Bid-bids$Engr.Est)/nrow(bids) 
mean(bids$Second.Bid) 
a = round(cor(bids$Second.Bid, bids$Engr.Est),5) 
summary(lm(bids$Second.Bid ~ bids$Engr.Est)) 
```

How much inconsistency does that represent? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(bids, aes(x=log(Second.Bid), y=log(Engr.Est))) + geom_point(size=2,shape=1) + 
  annotate(geom="text", x=12.5, y=19.5, label="Correlation", #"Scatter plot"
              color="red") +
  annotate(geom="text", x=12.5, y=19, label=a, #"Scatter plot"
              color="red")
```

The in-house engineers' guesses predict more than 98% of the variation in second-lowest bids. Some of the remaining variation may be explained by institutional guesswork. 

```{r, echo=FALSE, include=TRUE}
# Summary stats to help visualize some obvious potential predictors 
loc.summary = aggregate(accuracy ~ Loc, mean, data=na.omit(bids)) 
type.summary = aggregate(accuracy ~ Typeology, mean, data=na.omit(bids)) 
format.summary = aggregate(accuracy ~ Format, mean, data=na.omit(bids)) 
decile.summary = aggregate(accuracy ~ quantile, mean, data=na.omit(bids)) 
ld.summary = aggregate(accuracy ~ LD, mean, data=na.omit(bids)) 
```

Uncontrolled bivariate relationships provide easy clues as to predictors' potential role in more controlled multivariate relationships. Plots for this and subsequent exploratory work is at the end of the document; all consider accuracy, defined here as a ratio of the second-highest bid (a rule-of-thumb target) over the internal agency estimate.

- Location may matter: projects that span the Hudson River seem to come in, on average, higher than expected. Projects that span the Hudson River wind up costing (with respect to estimates) more, on average, than ones plunked squarely in either New York or New Jersey.
- The bidding process can be constrained or open, with potential ramifications on the ability to estimate costs. 
- The signal is stronger regarding the type of project, which has an identifiable (if yet uncontrolled) relationship with estimating accuracy. Of three categories — infrastructure, paving, building — the agency appears to predict paving projects the best. Note: the story changes when considering the lowest bid, where paving projects' relationship to estimates varies significantly. 
- Outliers for PATH and TB&T push the average accuracy metric up for each, otherwise there's little apparent difference across departments. 
- Strong signals emerge that projects of medium size (in dollar terms) may be less evasive than much larger or smaller projects. 

The implication of project size may provide the most valuable results. One might have expected the largest or smallest projects to be the tough to predict - particularly smaller projects. But it's the projects ranging from the 40th to 70th percentiles that seem to be the most challenging. 


#4. Modeling and prediction

Try and use exogenous covariates to predict an alternative engineering estimate, without using the low bid information, that might be closer to the low bid. 
Call it "expected low bid" or something so we can remember what we're trying to get. 

The data set carries dimensionality challenges, with a number of variables (absolutely and relative to the number of observations). It includes mostly continuous variables but also a number of qualitative factors, both ordinal and nominal and all treated categorically without conversion to binary subvariables - the modeling processes used here do that automatically. 


##4a. Base model (manual selection).
Interpretation: specification was manual and intuitive. Given the fact that estimators' already try and take much of this information into account, however, a model with even a handful of extra covariates could represent overfitting - trying to hard. 

###Note
Ensure the accuracy variable calculated earlier is dropped before modeling or introducesome dual (reverse) causality, which could confuse models.
###Note 2
When a number appears in the output without context, it is likely an information criterion (and AIC), which may or may not provide value post-modeling.

Split data into training  / test sets. 
```{r echo=TRUE, results="hide", message = FALSE, warning = FALSE}
train = subset(bids,bids$Date<"2019-03-31") 
test = subset(bids,bids$Date>="2019-04-01") 
```

```{r, echo=FALSE}
# Remove accuracy for modeling purposes. 
train$accuracy=NULL 
test$accuracy=NULL 
```

Choose a handful of potential predictors and build a linear model. 

```{r, include=TRUE, echo=TRUE, message = FALSE, warning = FALSE}
base = lm(Second.Bid ~ Engr.Est + Employment.in.construction + Format2 + Typeology + permits_1 + cci + quantile, data = train) 

  options(scipen=999)
  summary(base) 
  mse_b = round(mse(train$Second.Bid,base$fitted.values),0)
  adjr_b = round(summary(base)$adj.r.squared,3)
```

The equation above throws a decent amount of information at the engineering estimate and tries to predict the second bid. If the result is noticably closer to the low qualifying bid than the original estimate, you can use the delta as a post-estimation fudge factor to adjust the final estimate.

The numbers calculated above include a few metrics to use in comparing the three estimates against the objective data point at the low bid, which is what we're trying to predict. Ghose three estimates I'm talking about are: 
  1. The original, raw engineering cost estimate,
  2. The first alternative, where we built a model by hand to try and use a few more data points to enhance the original estimate, and; 
  3. The second alternative, a kitchen sink model that throws even more data points at the question. This followed an effort to use a penalized regression to identify the best covariates, but that penalization algorithm actually suggested there isn't much we can do to enhance the original estimate. (Note: this will prove prescient.)

I'll build a table near the very end of this script that summarize the metrics I'm using to understand how well these modeling efforts work. The metrics will be: 
  A. A basic t-test to understand whether there's even a statistically significant difference between the estimate I'm getting and the enhanced estimate I'm modeling with it, 
  B. A correlation between the two numbers, to try and understand the magnitude of that difference (if we can trust it really exists), 
  C. Two measures of the predictive modeling power of the models, an adjusted R-squared and the mean squared error (MSE). Both are common metrics of power. The first can be viewed discretely for each model but the second only provides a relative measure between models.

What is the summary of the predicted values? How does it compare to the summary of second-lowest bids?

```{r, echo=FALSE, include=FALSE, message = FALSE, warning = FALSE}
summary(base$fitted.values) 
summary(train$Second.Bid) 
```

Looking at the two summaries above, what's more accurate, the original engineering estimate or the first enhanced prediction? Neither, really. In fact the estimate and prediction aren't even (statistically) significantly different: 

```{r, echo=FALSE,include=TRUE}
summary(lm(base$fitted.values ~ train$Second.Bid))
```

Maybe something more robust can come with a little creativity. 

(Note: the model should control to prevent negative values. To be done next time.)

It might have been worth trying with the log of prices, only because the statistical fit becomes multiplicative instead of linear, but that produced similar results.

##4b. Regularization.
Strip the bids data of unusable stuff, then set controls and run.
The package I'm using, glmnet, requires a little extra preparation.

Could running a regularization process weed out weaker variables and identify one or two items that help prediction? The process would shrink (toward zero) the estimates for covariates that threaten to introduce more uncertainty than predictive power. This chooses a model automatically. 

Running a regularized regression below adopts base assumptions in the modeling software package (10 folds, standardized coefficients, gaussian distribution, MSE evaluation metric, et cetera). 

```{r, echo=FALSE, include=FALSE, message = FALSE, warning = FALSE}
train2 = train[,-c(1:5,7:10,12,14,16,53)] 
  train2 = na.omit(train2) 
test2 = test[,-c(1:5,7:10,12,14,16,53)] 

XP=data.matrix(train2) #XP=data.matrix(train2[,-3]) 
YP=data.matrix(train$Second.Bid) 
set.seed(1) 
lasso=cv.glmnet(x=XP, y=YP, alpha=1) 
```

How many variables survive the penalty process as the penalty grows? 

```{r, echo=FALSE}
plot(lasso) 
```

At a lower penalty (higher tuning parameter) there are a dozen non-zero predictors remaining, once the parameter cranks up a little all but one or two fall away. 

```{r, echo=FALSE, include=FALSE}
coef(lasso, s=lasso$lambda.1se) 
```

Initial attempt supresses all covariates. A little fine-tuning identifies the two parameters that survive as the model gets further from ordinary least squares without disappearing: 

```{r, echo=TRUE, include=TRUE}
lasso2 = cv.glmnet(x=XP, y=YP, alpha=.25) 
coef(lasso2, s=lasso2$lambda.1se) 
```


#5. Discussion.
This suggests using anything beyond the engineer's estimate itself to better predict the lowest qualifying good adds more uncertainty (in the form of noise that's tough to explain) than it adds value. (The "penalty" associated with adding variables is greater than the extra predictive power they bring.) The most obvious exception is project size, which isn't too surprising given the exploratory work done earlier. 

```{r, echo=FALSE, include=TRUE}
bids = fastDummies::dummy_cols(bids, select_columns = "quantile") 
bids$quantile = as.factor(bids$quantile) 
```
```{r, echo=TRUE, include=TRUE}
summary(lm(bids$Second.Bid ~ bids$Engr.Est + bids$quantile_2 + bids$quantile_3 + bids$quantile_4 + bids$quantile_5 + bids$quantile_6 + bids$quantile_7 + 
             bids$quantile_8 + bids$quantile_9 + bids$quantile_10)) 
```



#Exploratory work.

Plots and output from earlier exploratory bivariate work are below. Each considers a potential predictor's relationship to the agency's cost estimation accuracy, defined here as the second-lowest bid over the internal agency estimate. 

###Location.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ bids$Loc)) 

ggplot(bids, aes(x = Loc, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=loc.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
    ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

###Bidding process.
```{r, echo=FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$Format))) 

ggplot(bids, aes(x = Format, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=format.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

###Project typeology. 
```{r, echo = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$Typeology))) 

ggplot(bids, aes(x = Typeology, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=type.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

###Department.
```{r, echo = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$LD))) 

ggplot(bids, aes(x = LD, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=ld.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

###Project size.
```{r, echo = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$quantile))) 

ggplot(bids, aes(x = as.factor(quantile), y = accuracy)) + 
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=decile.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))

options(warn=0)
```
