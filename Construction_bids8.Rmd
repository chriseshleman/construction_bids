---
title: "Predicting costs"
author: "Chris Eshleman"
date: "10/20/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
---

\pagenumbering{arabic}

Q: What's the cross-validation being used? 
Q: Can I relax the penalty I'm applying via lasso? YES - THERE'S 
SOMETHING TO THIS - Flesh it out! 

The agency estimates project costs internally. It may be able to use statistics to improve those predictions. 

Model variation in bids, withholding the two most recent quarters of observations. Then test those models on the withheld observations. How well do they help with predictions? 

#Methods and Data.
  1. Basic (manually constructed linear model), 
  2. Lasso (penalized regression - machine learning). 
  
The project-level data comes from internal agency cost estimation.  
The economic data is just quarterly stuff from the usual suspects and is specific to national and regional economic and labor market conditions. 

Load some programming tools that are commonly used for this analysis. Not all of these packages will be used, and at some point it'll be worth backing up and cleaning the list. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
rm(list = ls()) # clear global environment 
cat("\014") # clear the console 

library(olsrr) 
library(data.table) 
library(ggplot2) 
library(lubridate) 
library(doBy) 
library(DataCombine) 
library(dplyr) 
library(compare) 
library(StatMeasures) 
library(caret) 
library(mlbench) 
library(glmnet) 
library(tidyr) 
library(tidyverse) 
library(broom) 
library(tidytext) 
library(widyr) 
library(Matrix) 
library(ggrepel) 
library(MLmetrics) 
library(mltools) 
```

#Overview. 
This effort bridges exploration of the agency's data with potential predictors from exogenous sources. 
The key idea is that if economic indicators (numbers) can add measurable value to the agency's cost estimation methods, it can help set the agency up for better informed next steps. Those next steps are yet to be defined. 

```{r}
#![Here's what we have in mind.](Estimator_Data.png)
```

The measures of "accuracy" are, for now, bivariate correlation. We discuss below why it's not yet time for fancier metrics, but the process above represents a first cut at trying to add a little more statistical value to the process. 

Data comes from a few sources and needs to be merged. Two usual suspects are the agency's internal project information and a set of economic indicators specific to Greater New York (18 counties on both sides of the Hudson River). 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
#setwd("C:/Users/ceshleman/Dropbox/Work and research/Port Authority/construction_bids")
setwd("~/Dropbox/Work and research/Port Authority/construction_bids") 
bids = read.csv("./Bid data 2019q3.csv") 
bids$Year = as.factor(bids$Year) 

bids$Date = as.Date(bids$Date,"%m/%d/%Y") 
bids = bids[order(bids$Date),] # Sort things by date 
bids$Engr.Est = as.numeric(bids$Engr.Est) 

bids$Quarter = quarter(bids$Date) 
bids$Quarter = paste("Q",quarter(bids$Date), sep="") 
bids$Q = paste(bids$Year,bids$Quarter,sep="-") 
```

Add economic data. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
econ = read.csv("./Economics 2019Q2.csv") 
econ$Q = paste(econ$Year,econ$Quarter,sep="-")

econ$Year = NULL  
econ$Quarter = NULL 
bids$Year = NULL 
bids$Quarter = NULL 

bids = merge(bids,econ, by = "Q", all.x=TRUE) 

rm(econ) 
```

## Add permits and steel prices.
The City of New York's database on permitting covers comercial and residential activity. 

I'd like to ask have robust data on the rest of the region, including 
(and namely) Jersey City, but it's weaker than the City's, which by itself is 
a decent barometer of construction activity in greater New York. 

Prices of construction materials and labor also figure into the agency's internal cost estimation, and I'll use steel prices for now. Future models might try and rope in other pricing data points, but the estimators are generally already taking prices into account when setting their numbers so this is likely of second- and third-order importance but the modeling selection algorithms may suggest using them. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
permits_short = read.csv("./Permits_short.csv") 
names(permits_short) 
permits_short$X=NULL 
names(permits_short) = c("Q","permits","permits_1") 

bids = merge(bids, permits_short, by = "Q", all.x=TRUE) 
rm(permits_short) 
``` 

The Engineering News-Record tracks and aggregates construction cost data through an index. Use that to cover prices. 

```{r}
cci = read.csv("./cci.csv") 
names(cci) = tolower(names(cci)) 
cci$avg. = NULL 
cci = gather(cci, month, cci, jan:dec, factor_key=TRUE) 
cci$month = gsub("(^[[:alpha:]])", "\\U\\1", cci$month, perl=TRUE)
cci$month = as.Date(paste(cci$month,"01",cci$year, sep="-"), format="%b-%d-%Y") 
cci = cci[month(cci$month) == 2 | month(cci$month) == 5 | month(cci$month) == 8 | month(cci$month) == 11, ] 

cci$Quarter = paste("Q",quarter(cci$month), sep="") 
cci$Q = paste(year(cci$month),cci$Quarter,sep="-") 
cci$Quarter = NULL 
cci$month = NULL 
cci$year=NULL 

bids = merge(bids, cci, by = "Q", all.x=TRUE) 

rm(cci) 
```


#Munge 
Stats software treats different variables in different ways depending on individal formatting. It's worth taking a look at the data structure. 

I'll need to tell the software to reformat some of the variables, namely the economic indicators. 

```{r, echo=FALSE, include=TRUE, message = FALSE, warning = FALSE}
bids$Employment.in.construction = as.numeric(as.character(bids$Employment.in.construction)) 
bids$Output.in.construction = as.numeric(as.character(bids$Output.in.construction)) 
bids$Total.population = as.numeric(as.character(bids$Total.population)) 
bids$Total.employment = as.numeric(as.character(bids$Total.employment)) 
bids$Total.output = as.numeric(as.character(bids$Total.output)) 
```

One variable of interest is the bidding process. Institutional discussions and earlier modeling suggests the bidding process may influence the bids. Limits placed on the range of bidders, for example, could, on average and holding other things constant, increase the average (and lowest qualifying) bid - this is basic microeconomics. I'll simplify the bidding format variable by making it binary: "public" for projects without significant constraints and "other" for ones, such as projects closed to firms not deemed "small business enterprises," that aren't. First I'll clean it a bit to consolidate near-dupliate categories. 

```{r echo=FALSE}
bids$Format = as.factor(ifelse(bids$Format=="Public","Public", ifelse(bids$Format=="D-B"|bids$Format=="M/WBE"|bids$Format=="MBE"|bids$Format=="MBE/WBE"|bids$Format=="MWBE", "M/WBE",ifelse(bids$Format=="SBE","SBE","Other"))))
bids$Format2 = as.factor(ifelse(bids$Format=="Public","Public","Other")) 
```

There's room to also eventually include the names (anonymized is fine) of each project estimator to help modeling. Past work has suggested there isn't major causal variation between estimators — they generally do a pretty equivalent job in estimating bids. But having their names included nonetheless may prove to offer some control value. We can leave that to future modeling. 

#Restating objective. 

We want to understand whether and how we might help the agency estimate the actual cost of a project. That's invariably going to be represented by the low qualifying bid, and our starting point is the estimate coming from the Engineering Department. 
From here on we'll define "accuracy" as the ratio of dollars estimated over dollars bid. So a "1" would mean the engineering team nailed it, a "0.94" would mean they estimated 94 cents for every 1 dollar in the low bid, et cetera. 

```{r echo=FALSE}
bids$accuracy = bids$Second.Bid / bids$Engr.Est 
```

If there are outliers in there - projects that, for an unexplainable reason, was way off, consider removing it. 

```{r echo=FALSE}
#bids = subset(bids,bids$accuracy<3) # remove major outliers 
```

We may want to think of project size categorically. 
```{r}
#bids$decile = decile(vector = bids$Second.Bid) 
#bids$decile = ordered(bids$decile, levels = 1:10)
bids = bids %>%
  mutate(quantile = ntile(as.numeric(Second.Bid), 10))
```

Back up data. 
```{r}
bid = bids # backup my data frame 
```


#Analysis
Now the data is prepped. 
So split it into the training  / test sets we talked about at the start. The bids start in 2015. 
  
```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
train = subset(bids,bids$Date<"2019-03-31") 
test = subset(bids,bids$Date>="2019-04-01") 
```

# Motivation. 
Why do we think developing a conrolled multivariate (complicated) model will be worth it? Well, the average gap is $2.5 million, or 20%, off of our estimates. What's the raw (uncontrolled) bivariate relationship between engineering estimates and low bids? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
a = round(cor(bids$Second.Bid, bids$Engr.Est),5) 

summary(lm(bids$Second.Bid ~ bids$Engr.Est)) 

ggplot(bids, aes(x=log(Second.Bid), y=log(Engr.Est))) + geom_point(size=2,shape=1) + 
  annotate(geom="text", x=12, y=19.5, label="Correlation", #"Scatter plot"
              color="red") +
  annotate(geom="text", x=12, y=19, label=a, #"Scatter plot"
              color="red")
```

(The logarithm treatment is just to distribute it across the plot (one of the observations is an outlier).) 

The in-house engineers' guesses predict more than 98% of the variation in low bids. 

Some of the remaining variation can be explained with some guesswork. Calculate summary stats for key variables - this will help later: 

```{r}
loc.summary = aggregate(accuracy ~ Loc, mean, data=na.omit(bids)) 
type.summary = aggregate(accuracy ~ Typeology, mean, data=na.omit(bids)) 
format.summary = aggregate(accuracy ~ Format, mean, data=na.omit(bids)) 
decile.summary = aggregate(accuracy ~ quantile, mean, data=na.omit(bids)) 
ld.summary = aggregate(accuracy ~ LD, mean, data=na.omit(bids)) 
```

Location likely has some predictive power that engineers may not be able to capture or fully predict. Basically, projects that span the Hudson River wind up costing more, on average, than ones plunked squarely in either New York or New Jersey. What's the raw (uncontrolled) relationship between bid accuracy and location? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ bids$Loc)) 

ggplot(bids, aes(x = Loc, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=loc.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
    ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```


A project's bidding process can be constrained or open, with potential ramifications on the ability to estimate costs: 

```{r}
summary(lm(bids$accuracy ~ as.factor(bids$Format))) 

ggplot(bids, aes(x = Format, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=format.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```


The signal is stronger regarding the type of project, which has an identifiable (if yet uncontrolled) relationship with estimating accuracy. What's the raw (uncontrolled) relationship between estimation accuracy and the type of project? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$Typeology))) 

ggplot(bids, aes(x = Typeology, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=type.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

By line department: 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$LD))) 

ggplot(bids, aes(x = LD, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=ld.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

Infrastructure projects for TB&T seem difficult to target, but the results aren't systematically high or low, just spread across a wide range:

```{r}
ggplot(bids, aes(x = LD, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=ld.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
    facet_wrap( ~ Typeology, ncol=2) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

We see signals that projects of medium size (in dollar terms) may be less evasive than much larger or smaller projects. What's the relationship between low bids and accuracy, when we start considering the size of low bids? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$quantile))) 

ggplot(bids, aes(x = as.factor(quantile), y = accuracy)) + 
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=decile.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid ÷ Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

Visualizing and testing the data iteratively like this has offered some initial insight into what might be accounting for variation in accuracy. 


# Modeling and prediction

Try and use exogenous covariates to predict an alternative engineering estimate, without using the low bid information, that might be closer to the low bid. 
Call it "expected low bid" or something so we can remember what we're trying to get. 

##A. Base model (manual selection) 
Interpretation: specification was manual and intuitive. 

Note: ensure the "accuracy" variable we calculated earlier is dropped before modeling or I'll be introducing some dual (reverse) causality, which will confuse the models. 

Note 2: when a number appears in the output without context, it is likely an information criterion (and AIC), which may or may not provide value post-modeling. 

First remove accuracy. 
```{r}
train$accuracy=NULL 
test$accuracy=NULL 
```

Run model. 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
base = lm(Second.Bid ~ Engr.Est + Employment.in.construction + Format2 + Typeology + permits_1 + cci + as.factor(quantile), data = train) 

  summary(base) 
  options(scipen=999)
  mse_b = round(mse(train$Second.Bid,base$fitted.values),0)
  adjr_b = round(summary(base)$adj.r.squared,3)
```

The equation above throws extra information at the engineering estimate and tries to predict the low qualifying bid. If the result is noticably closer to the low qualifying bid than the original estiamte, you can use the delta as a post-estimation fudge factor to adjust your final estimate.

The numbers calculated above include a few metrics to use in comparing the three estimates against the objective data point at the low bid, which is what we're trying to predict. Ghose three estimates I'm talking about are: 
  1. The original, raw engineering cost estimate,
  2. The first alternative, where we built a model by hand to try and use a few more data points to enhance the original estimate, and; 
  3. The second alternative, a kitchen sink model that throws even more data points at the question. This followed an effort to use a penalized regression to identify the best covariates, but that penalization algorithm actually suggested there isn't much we can do to enhance the original estimate. (Note: this will prove prescient.)

I'll build a table near the very end of this script that summarize the metrics I'm using to understand how well these modeling efforts work. The metrics will be: 
  A. A basic t-test to understand whether there's even a statistically significant difference between the estimate I'm getting and the enhanced estimate I'm modeling with it, 
  B. A correlation between the two numbers, to try and understand the magnitude of that difference (if we can trust it really exists), 
  C. Two measures of the predictive modeling power of the models, an adjusted R-squared and the mean squared error (MSE). Both are common metrics of power. The first can be viewed discretely for each model but the second only provides a relative measure between models. 
  
What is the summary of the predicted values? 
How does it compare to the summary of low bids? 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
summary(base$fitted.values) 
summary(train$Second.Bid) 
```

Looking at the two summaries above, what's more accurate, the original engineering estimate or the first enhanced prediction? Neither, really. In fact the estimate and prediction aren't even (statistically) significantly different. Maybe something more robust can come with a little creativity. 

(Note: the model should control to prevent negative values. To be done next time.)

It might have been worth trying with the log of prices, only because the statistical fit becomes multiplicative instead of linear, but that produced similar results. 


## B. Machine learning
Strip the bids data of unusable stuff, then set controls and run.
The package I'm using, glmnet, requires a little extra preparation.

The output below represents the algorithm's effort to look for variatbles that might be dependable in adding predictive power to the original engineering estimate. 

Running a regularized regression below adopts base assumptions in the modeling software package (10 folds, standardized coefficients, gaussian distribution, MSE evaluation metric, et cetera). 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
train2 = train[,-c(1:5,7:10,12,14,52)] 
  train2 = na.omit(train2) 
test2 = test[,-c(1:5,7:10,12,14,52)] 

XP=data.matrix(train2) #XP=data.matrix(train2[,-3]) 
YP=data.matrix(train$Second.Bid) 
set.seed(1) 
lasso=cv.glmnet(x=XP, y=YP, alpha=1) 
```

How many variables survive the penalty process as the penalty grows? 

```{r}
plot(lasso) 
```

At a lower penalty (higher tuning parameter) there are a dozen non-zero predictors remaining, once the parameter cranks up a little all but a couple fall away. A little fine-tuning identifies the two parameters that survive as the model gets further from ordinary: 

```{r}
lasso2 = cv.glmnet(x=XP, y=YP, alpha=.25) 
coef(lasso2, s=lasso2$lambda.1se) 
```

This suggests using anything beyond the engineer's estimate itself to better predict the lowest qualifying good adds more uncertainty (in the form of noise that's tough to explain) than it adds value. (The "penalty" associated with adding variables is greater than the extra predictive power they bring.) The most obvious exception is project size, which isn't too surprising given the exploratory work done earlier. 

```{r}
bids = fastDummies::dummy_cols(bids, select_columns = "quantile") 
summary(lm(bids$Second.Bid ~ bids$Engr.Est + bids$quantile_2 + bids$quantile_3 + bids$quantile_4 + bids$quantile_5 + bids$quantile_6 + bids$quantile_7 + 
             bids$quantile_8 + bids$quantile_9 + bids$quantile_10)) 
```

