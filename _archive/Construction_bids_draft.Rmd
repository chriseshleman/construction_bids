---
title: "Predicting costs"
author: ""
date: "10/25/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
---

\pagenumbering{arabic}

#1. Overview
The agency estimates project costs internally prior to public bidding openings. 

Model variation in those bids a couple of ways and try to find ways to improve upon the original estimates before future bidding openings. (Compare basic statistical methods with what are called "regularization" alternatives.)

Project-level data includes a number of characteristics. Economic and demographic variables are specific to national and regional economic and labor market conditions. City of New York building permits can help proxy for activity in the broader regional construction market. ENR's cost index serves as proxy for broader hard construction costs.

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
rm(list = ls()) # clear global environment 
cat("\014") # clear the console 
options(warn=-1)

# A number of programming tools commonly used for this analysis are loaded below; not all will be used - some are relics from early iterations of this work.

library(olsrr) 
library(data.table) 
library(ggplot2) 
library(lubridate) 
library(doBy) 
library(DataCombine) 
library(dplyr) 
library(compare) 
library(StatMeasures) 
library(caret) 
library(mlbench) 
library(glmnet) 
library(tidyr) 
library(tidyverse) 
library(broom) 
library(tidytext) 
library(widyr) 
library(Matrix) 
library(ggrepel) 
library(MLmetrics) 
library(mltools) 
library(grid) 
```

The goal is to bridge exploration of the agency's data with potential exogenous predictors, some of which the internal estimation process may underestimate or inadvertently miss. If those predictors can add measurable value to the agency's cost estimation methods it may inform potential changes in engineers' estimation methods.


#2. Data. 
Project data, called "bids" here, come from the Engineering Department. Economic and demographic indicators are specific to Greater New York (18 counties on both sides of the Hudson River) and come from the Planning and Regional Development Department; underlying data is from Oxford Economics.

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
setwd("~/Dropbox/Work and research/Port Authority/construction_bids") 
bids = read.csv("./Bid data 2019q3.csv") 
bids$Year = as.factor(bids$Year) 

bids$Date = as.Date(bids$Date,"%m/%d/%y") 
bids = bids[order(bids$Date),] # Sort things by date 
bids$Engr.Est = as.numeric(bids$Engr.Est) 

bids$Quarter = quarter(bids$Date) 
bids$Quarter = paste("Q",quarter(bids$Date), sep="") 
bids$Q = paste(bids$Year,bids$Quarter,sep="-") 
```

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
econ = read.csv("./Economics 2019Q2.csv") 
econ$Q = paste(econ$Year,econ$Quarter,sep="-")

econ$Year = NULL  
econ$Quarter = NULL 
bids$Year = NULL 
bids$Quarter = NULL 

bids = merge(bids,econ, by = "Q", all.x=TRUE) 
rm(econ) 
```

Construction data is better from some parts of the region than others, but Jersey City's construction data is not yet as dependable as the City of New York's. NYC dominates regional construction anyway and it's justifiable for now to use its permitting data as somewhat representative of the broader regional construction market. 

Prices of construction materials and labor already figure directly into the agency's internal cost estimation, and this analysis borrows the same index for predictive powers despite uncertainty regarding whether its implicit presence in agency estimates helps or hurts its role in any multivariable treatments.  It is likely of second-order importance for now. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
permits_short = read.csv("./Permits_short.csv") 
permits_short$X=NULL 
names(permits_short) = c("Q","permits","permits_1") 

bids = merge(bids, permits_short, by = "Q", all.x=TRUE) 
rm(permits_short) 
``` 

```{r, echo=FALSE, include=FALSE}
cci = read.csv("./cci.csv") 
names(cci) = tolower(names(cci)) 
cci$avg. = NULL 
cci = gather(cci, month, cci, jan:dec, factor_key=TRUE) 
cci$month = gsub("(^[[:alpha:]])", "\\U\\1", cci$month, perl=TRUE)
cci$month = as.Date(paste(cci$month,"01",cci$year, sep="-"), format="%b-%d-%Y") 
cci = cci[month(cci$month) == 2 | month(cci$month) == 5 | month(cci$month) == 8 | month(cci$month) == 11, ] 

cci$Quarter = paste("Q",quarter(cci$month), sep="") 
cci$Q = paste(year(cci$month),cci$Quarter,sep="-") 
cci$Quarter = NULL 
cci$month = NULL 
cci$year=NULL 

bids = merge(bids, cci, by = "Q", all.x=TRUE) 

rm(cci) 
```

One variable of interest is the actual bidding process. Institutional discussions and earlier modeling suggests the bidding process may influence actual project costs. Limits placed on the range of bidders could, for example, on average and holding other things constant, increase the average (and lowest qualifying) bid - this is basic microeconomics. I'll simplify the bidding format variable by making it binary: "public" for projects without significant constraints and "other" for ones closed to firms that don't qualify, such as large enterprises. First I'll clean that variable a bit to consolidate categories very close in spirit. 

```{r echo=FALSE, include=FALSE}
bids$Format = as.factor(ifelse(bids$Format=="Public","Public", ifelse(bids$Format=="D-B"|bids$Format=="M/WBE"|bids$Format=="MBE"|bids$Format=="MBE/WBE"|bids$Format=="MWBE", "M/WBE",ifelse(bids$Format=="SBE","SBE","Other"))))
bids$Format2 = as.factor(ifelse(bids$Format=="Public","Public","Other")) 
```

Earlier work suggests there isn't major variation across the individual developing the in-house estimate and unique employee (estimator) identifiers are omitted from this review.

Agency projects last for months or years and actual costs do not exist for many of the observations, which at just over 260 projects already creates minor dimensionality concerns given the number of potential predictive variables. The second-lowest qualifying bid provides a reasonable target for evaluating internal estimates. The accuracy metric referenced through the exploratory discussion below and appendixed plots represents a ratio of that second-lowest bid over the estimate, both in dollars. A 1 would represent a case where the internal estimate (denominator) precisely matched the second-lowest bid; a 0.94 would mean the bidder bid 94 cents for every dollar estimated internally, et cetera. 

```{r, echo=TRUE, include=FALSE}
bids$accuracy = bids$Second.Bid / bids$Engr.Est 
```

```{r, echo=FALSE}
# Project size can be considered categorically. 
bids = bids %>%
  mutate(quantile = ntile(as.numeric(Second.Bid), 10))
bids$quantile = as.factor(bids$quantile) 
bids$quantile = ordered(bids$quantile, levels = 1:10) 
```


```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Reformat some of the variables, namely the economic indicators, to accommodate programming platform.  
bids$Employment.in.construction = as.numeric(as.character(bids$Employment.in.construction)) 
bids$Output.in.construction = as.numeric(as.character(bids$Output.in.construction))
bids$Total.population = as.numeric(as.character(bids$Total.population)) 
bids$Total.employment = as.numeric(as.character(bids$Total.employment)) 
bids$Total.output = as.numeric(as.character(bids$Total.output)) 
```

```{r, echo=FALSE}
bid = bids # backup my data frame 
```


#3. Exploratory analysis. 

Why might developing a conrolled multivariate model will be worth it? The average gap between bids and estimates is less than $900,000, or around 5% (the average project bid was $15 million). 

```{r, echo=FALSE, include=FALSE}
weighted.mean((bids$Second.Bid/bids$Engr.Est), bids$Engr.Est, na.rm = TRUE)
sum(bids$Second.Bid-bids$Engr.Est)/nrow(bids) 
mean(bids$Second.Bid) 
a = round(cor(bids$Second.Bid, bids$Engr.Est),5) 
summary(lm(bids$Second.Bid ~ bids$Engr.Est)) 
```

How much inconsistency does that represent? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(bids, aes(x=log(Second.Bid), y=log(Engr.Est))) + geom_point(size=2,shape=1) +
  annotate(geom="text", x=12.5, y=19.5, label="Correlation", #"Scatter plot"
              color="red") +
  annotate(geom="text", x=12.5, y=19, label=a, #"Scatter plot"
              color="red")
```

The in-house engineers' guesses predict more than 98% of the variation in second-lowest bids. Some of the remaining variation may be explained by institutional guesswork. 

```{r, echo=FALSE, include=TRUE}
# Summary stats to help visualize some obvious potential predictors 
loc.summary = aggregate(accuracy ~ Loc, mean, data=na.omit(bids)) 
type.summary = aggregate(accuracy ~ Typeology, mean, data=na.omit(bids)) 
format.summary = aggregate(accuracy ~ Format, mean, data=na.omit(bids)) 
decile.summary = aggregate(accuracy ~ quantile, mean, data=na.omit(bids)) 
ld.summary = aggregate(accuracy ~ LD, mean, data=na.omit(bids)) 
```

Uncontrolled bivariate relationships provide easy clues as to predictors' potential role in more controlled multivariate relationships. Plots for this exploratory work are in the appendix at the end of the document; all use the definition of "accuracy" defined a few paragraphs above.

- Location may matter: projects that span the Hudson River seem to come in, on average, higher than expected. Projects that span the Hudson River wind up costing (with respect to estimates) more, on average, than ones plunked squarely in either New York or New Jersey.
- The bidding process can be constrained or open, with potential ramifications on the ability to estimate costs. (Visualization for that relationship is just below these bullet points. Plots for the other bullet points can be found in the appendix, along with corresponding test statistics.)
- The signal is stronger regarding the type of project, which has an identifiable (if yet uncontrolled) relationship with estimating accuracy. Of three categories â€” infrastructure, paving, building â€” the agency appears to predict paving projects the best. Note: the story changes when considering the lowest bid, where paving projects' relationship to estimates varies significantly. 
- Outliers for PATH and TB&T push the average accuracy metric up for each, otherwise there's little apparent difference across departments. 
- Strong signals emerge that projects of medium size (in dollar terms) may be less evasive than much larger or smaller projects. 

##Bidding process (format).
```{r, echo=FALSE}
ggplot(bids, aes(x = Format, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=format.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10))
```

The implication of project size may provide the most valuable results. One might have expected the largest or smallest projects to be the toughest to predict - particularly the very smallest projects. But it's the projects ranging from the 40th to 70th percentiles that seem to be the most challenging. 


#4. Modeling and prediction

Try and use exogenous covariates to predict an alternative engineering estimate, without using the second lowest bid information, that might be closer to the low bid. (Note: when projects only attracted one bid, that bid is used instead of a second-lowest bid. There are few of these projects in the data set.)

The data set carries dimensionality challenges, with a number of variables (absolutely and relative to the number of observations). It includes mostly continuous variables but also a number of qualitative factors, both ordinal and nominal and all treated categorically without conversion to binary subvariables - the modeling processes used here do that automatically. 


##4a. Base model (manual selection).
Interpretation: specification was manual and intuitive. Given the fact that estimators' already try and take much of this information into account, however, a model with even a handful of extra covariates could represent overfitting - trying to hard. 

Note: ensure the accuracy variable calculated earlier is dropped before modeling or introduce some dual (reverse) causality, which could confuse models.
Note: when a number appears in the output without context, it is likely an information criterion (and AIC), which may or may not provide value post-modeling.

Split data into training  / test sets. And, just for modeling purposes, remove the accuracy metric. One would split randomly with cross-sectional data and since this effort aims to help predict temporally, split by date â€” use projects (n=213) from 2015 through the first quarter of 2019 to train models and projects (n=26) since the end of March to test them. That's not many projects for testing but the real test will come as another quarter or two of data rolls in. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
train = subset(bids,bids$Date<"2019-03-31") 
test = subset(bids,bids$Date>="2019-04-01")
```

```{r, echo=FALSE}
train$accuracy=NULL 
test$accuracy=NULL 
```

Choose a handful of potential predictors and build a linear model. Go with the original estimate (always included) and (1) regional construction-specific employment, (2) bidding process (public or other), (3) typeology (infrastructure, building, or paving), (4) construction permits (lagged by one quarter), (5) construction costs (indexed), and (6) project size, with project dollars (from the second bid) categorized in deciles. 

```{r, include=TRUE, echo=TRUE, message = FALSE, warning = FALSE}
base = lm(Second.Bid ~ Engr.Est + Employment.in.construction + Format2 + Typeology + permits_1 + cci + quantile, data = train) 

  options(scipen=999)
  summary(base) 
```

The equation above throws a decent amount of information at the engineering estimate and tries to predict the second bid. If the result is noticably closer to the low qualifying bid than the original estimate, you can use the delta as a post-estimation fudge factor to adjust the final estimate.

What is the summary of the predicted values? How does it compare to the summary of second-lowest bids? Looking at the two summaries, the original engineering estimate and the enhanced prediction do appear to be significantly (statistically) different. 
```{r, echo=FALSE, include=FALSE, message = FALSE, warning = FALSE}
summary(lm(base$fitted.values ~ train$Second.Bid))
```
.

Maybe something more robust can come with a little creativity.


##4b. Shrinkage and automated variable selection.

Traditional methodology â€” tossing variables that made intuitive sense into a model and evaluating outcomes â€” might have identified a few reliable predictors. But it fell short and, even if it had, specification problems and covariate bias would have challenged the results.
A survival method may help â€” automated variable selection can weed out weaker variables and identify one or more key items to help prediction. Regularization processes penalize variables that threaten to introduce more uncertainty than predictive power to a model. Regularization modeling builds atop ordinary least squares regression, which fits a curve to data that minimizes the distance between the curve and any given point in the data set. Traditional least squares modeling can be prone to overfitting â€” reading too much signal from what is essentially meaningless noise and providing a tool that creates a poor fit for data that, while tied to the underlying process being modeled, was absent during the fitting process; this presents obvious challenges for prediction.

Regularization begins with the least squares method and adds a penalty. That penalty term is guided by a tuning parameter that essentially works as a dimmer switch â€” it can be cranked up to increase the penalty or down all the way to zero, which effectively turns off the penalty and produces the same results one would get from ordinary least squares. The tuning parameter is represented by the lambda in the second equation for error estimation (both from Hastie and Tibshirani):

```{r fig.width=2.5, fig.height=10,echo=FALSE}
library(png)
library(grid)
img <- readPNG("error_ols.png")
 grid.raster(img)
```

```{r fig.width=3.45, fig.height=10,echo=FALSE}
library(png)
library(grid)
img <- readPNG("error_ml3.png")
 grid.raster(img)
```

Run cross-validation to pick the tuning parameter. Cross-validation runs modeling processes iteratively by resampling the data â€” splitting it into a series of tranches used systematically as training and test sets. The fit with the lowest test error identifies the tuning parameter: 

```{r, echo=TRUE, include=FALSE, message = FALSE, warning = FALSE}
# Pare the data set to exclude variables inappropriate for the modeling process. 
blend = 1 # pick penalization process (between 0 and 1)

train2 = train[,-c(1:5,7:10,12,14,53)] 
  train2 = na.omit(train2) 
test2 = test[,-c(1:5,7:10,12,14,53)] 
  test2 = na.omit(test2) 
  x = train2[,-5]
  x = data.matrix(x) 
```
Note: to help here with manageability and reproducibility, adopt some of the modeling softwareâ€™s basic assumptions (10 folds, standardized coefficients, use of mean squared error as an evaluation metric, et cetera).

```{r, echo=FALSE, include=TRUE}
set.seed(423)
cv = cv.glmnet(x, train2[, 5], alpha = blend) 
plot(cv) 
```

The penalty term associated with an optimal tuning parameter (associated with the lowest model error) coincides with a model suppressing all but a handful of predictors.

This regularization process actually carries the prospect of increasing bias and, in the process, dulling coefficientsâ€™ reflection of the real world relationship between the processes being modeled. (It can introduce or exacerbate bias.) Regularization is often run with a penalty that, prior to tuning, sums squared coefficients or their absolute values, the second of which is responsible for forcing weak predictors completely out of the model. Performing analysis with the latter shrunk all covariates (aside from the engineerâ€™s estimate) to zero, suppressing them completely. This is a harsh variable selection process but it mightâ€™ve been expected given the original estimatesâ€™ proven strength. It doesnâ€™t leave enough clues for anyone to offer potential improvements. A little fine-tuning to the penalty combines the regularization treatments and gives the model a little leash, identifying predictors that survive as the model gets further from ordinary least squares without disappearing:

```{r, echo=FALSE, include=TRUE}
cv.model = glmnet(x, train2[, 5], alpha=blend, lambda=cv$lambda.min) 
```

```{r, echo=FALSE, include=TRUE}
cv.model 
#coef(cv.model) 
```

Predict using the model and the test data. 
```{r, echo=TRUE, include=FALSE}
cv.predict = predict(cv, s=cv$lambda.min ,newx=data.matrix(test2[,-5])) 
```


## 4c. Evaluate regularization. 

Compare this error with a basic OLS regression using all potential covariates; regularized interations can be compared with it. Turning the tuning parameter (the dimmer switch) off makes it easy.

```{r, echo=TRUE, include=FALSE}
ols.model = glmnet(x, train2[, 5], alpha=blend ,lambda=0) 
ols.predict = predict(ols.model, s=0, newx=data.matrix(test2[,-5])) 
```

Errors (test) for regularization and ordinary models, respectively: 
```{r, echo=FALSE, include=TRUE}
mean((cv.predict-test2$Second.Bid)^2) # Regularized error, test
mean((ols.predict-test2$Second.Bid)^2) # OLS error, test 
```

Regularization not only reduced error for the model built using pre-Q2 2019 projects, but that model then provides a stronger fit using the withheld projects.  And consider the implications for interpretability. Regress using the full data set: 
```{r, echo=FALSE, include=TRUE}
full2 = rbind(train2,test2) 
  x = train2[,-5]
  x = data.matrix(x) 
full.model = glmnet(data.matrix(full2[,-5]), full2[, 5], alpha=blend, lambda=cv$lambda.min) 
full.predict = predict(full.model, type="coefficients", s = cv$lambda.min) 
full.predict
```


#5. Discussion.
This suggests some room for targeted efforts to add value to the average engineer's estimate, since it already correlated very highly with targets. The most obvious predictor is project size, which isn't too surprising given the exploratory work done earlier. Next steps include looking for best practices regarding post-estimation adjustments for project cost by size. Here are bivariate, uncontrolled relationship parameters between bids and project size: 

```{r, echo=FALSE, include=TRUE}
bids = fastDummies::dummy_cols(bids, select_columns = "quantile") 
bids$quantile = as.factor(bids$quantile) 
```

```{r, echo=TRUE, include=TRUE}
summary(lm(bids$Second.Bid ~ bids$Engr.Est + bids$quantile_2 + bids$quantile_3 + bids$quantile_4 + bids$quantile_5 + bids$quantile_6 + bids$quantile_7 + 
             bids$quantile_8 + bids$quantile_9 + bids$quantile_10)) 
```

\pagebreak

#Appendix: exploratory work.

Plots and output from earlier exploratory bivariate work are below. Each considers a potential predictor's relationship to the agency's cost estimation accuracy, defined here as the second-lowest bid over the internal agency estimate. 

##Location.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
b = round(summary(aov(bids$accuracy ~ bids$Loc))[[1]][["Pr(>F)"]][[1]],4)
ggplot(bids, aes(x = Loc, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=loc.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
    ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10)) + 
    annotate(geom="text", x=2, y=3.5, label="p-value", 
              color="red") +
    annotate(geom="text", x=2, y=3.25, label=b, 
              color="red")
```

##Bidding process ("format").
```{r, echo=FALSE}
c = round(summary(aov(bids$accuracy ~ bids$Format))[[1]][["Pr(>F)"]][[1]],4)
ggplot(bids, aes(x = Format, y = accuracy)) + #, shape = Loc)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=format.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10)) + 
    annotate(geom="text", x=2, y=3.5, label="p-value", 
              color="red") +
    annotate(geom="text", x=2, y=3.25, label=c, 
              color="red")
```

##Project typeology.
```{r, echo = FALSE}
d = round(summary(aov(bids$accuracy ~ bids$Typeology))[[1]][["Pr(>F)"]][[1]],4)
ggplot(bids, aes(x = Typeology, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=type.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10)) + 
    annotate(geom="text", x=2, y=3.5, label="p-value", 
              color="red") +
    annotate(geom="text", x=2, y=3.25, label=d, 
              color="red")
```

##Department.
```{r, echo = FALSE}
e = round(summary(aov(bids$accuracy ~ bids$LD))[[1]][["Pr(>F)"]][[1]],4)
ggplot(bids, aes(x = LD, y = accuracy)) + #, shape = Typeology)) +  
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=ld.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10)) + 
      annotate(geom="text", x=2, y=3.5, label="p-value", 
              color="red") +
    annotate(geom="text", x=2, y=3.25, label=e, 
              color="red")
```

##Project size (dollars bid).
```{r, echo = FALSE}
f = round(summary(aov(bids$accuracy ~ bids$quantile))[[1]][["Pr(>F)"]][[1]],4)
ggplot(bids, aes(x = as.factor(quantile), y = accuracy)) + 
    geom_hline(colour="dark gray", yintercept=1) +
    geom_jitter(width=0.2) +
    geom_crossbar(data=decile.summary, aes(ymin = accuracy, ymax = accuracy),
                  size=1,col="red", width = .5) + 
      ylab("Second Bid Ã· Estimate") + 
    theme(axis.title.y=element_text(size=10)) + 
      annotate(geom="text", x=2, y=3.5, label="p-value", 
              color="red") +
    annotate(geom="text", x=2, y=3.25, label=f, 
              color="red")
```

```{r, echo=FALSE}
options(warn=0)
```

