---
title: "Predicting costs"
author: "Chris Eshleman"
date: "10/20/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
---

\pagenumbering{arabic}

Q: What's the cross-validation being used? 
Q: Can I relax the penalty I'm applying via lasso? YES - THERE'S 
SOMETHING TO THIS - Flesh it out! 

The agency estimates project costs internally. It may be able to use statistics to improve those predictions. 

Model variation in bids, withholding the two most recent quarters of observations. Then test those models on the withheld observations. How well do they help with predictions? 

#Methods and Data.
  1. Basic (manually constructed linear model), 
  2. Lasso (penalized regression - machine learning). 
  
The project-level data comes from internal agency cost estimation.  
The economic data is just quarterly stuff from the usual suspects and is specific to national and regional economic and labor market conditions. 

Load some programming tools that are commonly used for this analysis. Not all of these packages will be used, and at some point it'll be worth backing up and cleaning the list. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
rm(list = ls()) # clear global environment 
cat("\014") # clear the console 

library(olsrr) 
library(data.table) 
library(ggplot2) 
library(lubridate) 
library(doBy) 
library(DataCombine) 
library(dplyr) 
library(compare) 
library(StatMeasures) 
library(caret) 
library(mlbench) 
library(glmnet) 
library(tidyr) 
library(tidyverse) 
library(broom) 
library(tidytext) 
library(widyr) 
library(Matrix) 
library(ggrepel) 
library(MLmetrics) 
library(mltools) 
```

#Overview. 
This effort bridges exploration of the agency's data with potential predictors from exogenous sources. 
The key idea is that if economic indicators (numbers) can add measurable value to the agency's cost estimation methods, it can help set the agency up for better informed next steps. Those next steps are yet to be defined. 

```{r}
#![Here's what we have in mind.](Estimator_Data.png)
```

The measures of "accuracy" are, for now, bivariate correlation. We discuss below why it's not yet time for fancier metrics, but the process above represents a first cut at trying to add a little more statistical value to the process. 

Data comes from a few sources and needs to be merged. Two usual suspects are the agency's internal project information and a set of economic indicators specific to Greater New York (18 counties on both sides of the Hudson River). 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
#setwd("C:/Users/ceshleman/Dropbox/Work and research/Port Authority/construction_bids")
setwd("~/Dropbox/Work and research/Port Authority/construction_bids") 
bids = read.csv("./Bid data 2019q3.csv") 
bids$Year = as.factor(bids$Year) 
head(bids) 
bids$Date = as.Date(bids$Date,"%m/%d/%Y") 
bids = bids[order(bids$Date),] # Sort things by date 
bids$Engr.Est = as.numeric(bids$Engr.Est) 

bids$Quarter = quarter(bids$Date) 
bids$Quarter = paste("Q",quarter(bids$Date), sep="") 
bids$Q = paste(bids$Year,bids$Quarter,sep="-") 
```

Add economic data. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
econ = read.csv("./Economics 2019Q2.csv") 
econ$Q = paste(econ$Year,econ$Quarter,sep="-")

econ$Year = NULL  
econ$Quarter = NULL 
bids$Year = NULL 
bids$Quarter = NULL 

bids = merge(bids,econ, by = "Q", all.x=TRUE) 

rm(econ) 
```

## Add permits and steel prices.
The City of New York's database on permitting covers comercial and residential activity. 

I'd like to ask have robust data on the rest of the region, including 
(and namely) Jersey City, but it's weaker than the City's, which by itself is 
a decent barometer of construction activity in greater New York. 

Prices of construction materials and labor also figure into the agency's internal cost estimation, and I'll use steel prices for now. Future models might try and rope in other pricing data points, but the estimators are generally already taking prices into account when setting their numbers so this is likely of second- and third-order importance but the modeling selection algorithms may suggest using them. 

```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
permits_short = read.csv("./Permits_short.csv") 
names(permits_short) 
permits_short$X=NULL 
names(permits_short) = c("Q","permits","permits_1") 

bids = merge(bids, permits_short, by = "Q", all.x=TRUE) 
rm(permits_short) 
``` 

The Engineering News-Record tracks and aggregates construction cost data through an index. Use that to cover prices. 

```{r}
cci = read.csv("./cci.csv") 
names(cci) = tolower(names(cci)) 
cci$avg. = NULL 
cci = gather(cci, month, cci, jan:dec, factor_key=TRUE) 
cci$month = gsub("(^[[:alpha:]])", "\\U\\1", cci$month, perl=TRUE)
cci$month = as.Date(paste(cci$month,"01",cci$year, sep="-"), format="%b-%d-%Y") 
cci = cci[month(cci$month) == 2 | month(cci$month) == 5 | month(cci$month) == 8 | month(cci$month) == 11, ] 

cci$Quarter = paste("Q",quarter(cci$month), sep="") 
cci$Q = paste(year(cci$month),cci$Quarter,sep="-") 
cci$Quarter = NULL 
cci$month = NULL 
cci$year=NULL 

bids = merge(bids, cci, by = "Q", all.x=TRUE) 

rm(cci) 
```


#Munge 
Stats software treats different variables in different ways depending on individal formatting. It's worth taking a look at the data structure. 

I'll need to tell the software to reformat some of the variables, namely the economic indicators. 

```{r, echo=FALSE, include=TRUE, message = FALSE, warning = FALSE}
bids$Employment.in.construction = as.numeric(as.character(bids$Employment.in.construction)) 
bids$Output.in.construction = as.numeric(as.character(bids$Output.in.construction)) 
bids$Total.population = as.numeric(as.character(bids$Total.population)) 
bids$Total.employment = as.numeric(as.character(bids$Total.employment)) 
bids$Total.output = as.numeric(as.character(bids$Total.output)) 
```

One variable of interest is the bidding process. Institutional discussions and earlier modeling suggests the bidding process may influence the bids. Limits placed on the range of bidders, for example, could, on average and holding other things constant, increase the average (and lowest qualifying) bid - this is basic microeconomics. I'll simplify the bidding format variable by making it binary: "public" for projects without significant constraints and "other" for ones, such as projects closed to firms not deemed "small business enterprises," that aren't. 

```{r echo=FALSE}
bids$Format = as.factor(ifelse(bids$Format=="Public","Public","Other")) 
```

There's room to also eventually include the names (anonymized is fine) of each project estimator to help modeling. Past work has suggested there isn't major causal variation between estimators â€” they generally do a pretty equivalent job in estimating bids. But having their names included nonetheless may prove to offer some control value. We can leave that to future modeling. 

#Restating objective. 

We want to understand whether and how we might help the agency estimate the actual cost of a project. That's invariably going to be represented by the low qualifying bid, and our starting point is the estimate coming from the Engineering Department. 

From here on we'll define "accuracy" as the ratio of dollars estimated over dollars bid. So a "1" would mean the engineering team nailed it, a "0.94" would mean they estimated 94 cents for every 1 dollar in the low bid, et cetera. 

```{r echo=FALSE}
bids$accuracy = bids$Engr.Est / bids$Low.Bid 
```

I think there's a pesky outlier in there - a project that, for a strange reason (unexplainable), was way off. Suggest removing it. 

```{r echo=FALSE}
bids = subset(bids,bids$accuracy<3) # remove major outliers 
```

We may want to think of project size categorically. 
```{r}
bids$decile = decile(vector = bids$Low.Bid) 
bids$decile = ordered(bids$decile, levels = 1:10)
```

Back up data. 
```{r}
bid = bids # backup my data frame 
```


#Analysis
Now the data is prepped. 
So split it into the training  / test sets we talked about at the start. The bids start in 2015. 
  
```{r echo=FALSE, results="hide", message = FALSE, warning = FALSE}
train = subset(bids,bids$Date<"2019-03-31") 
test = subset(bids,bids$Date>="2019-04-01") 
```

# Motivation. 
Why do we think developing a conrolled multivariate (complicated) model will be worth it? Well, the average gap is $2.5 million, or 20%, off of our estimates. What's the raw (uncontrolled) bivariate relationship between engineering estimates and low bids? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
a = round(cor(bids$Low.Bid, bids$Engr.Est),5) 
ggplot(bids, aes(x=log(Low.Bid), y=log(Engr.Est))) + geom_point(size=2,shape=1) + annotate(geom="text", x=12, y=19, label=a, #"Scatter plot"
              color="red")

summary(lm(bids$Low.Bid ~ bids$Engr.Est)) 
```

(The logarithm is just to distribute it across the plot (one of the observations is an outlier).) 

The in-house engineers arguably do a pretty good job - they're guesses predict more than 98% of the variation in low bids. 

Some of the remaining variation can be explained with some guesswork. 

Location seems to have some predictive power. Not a ton, but a little. 
Basically, projects that somehow span the Hudson River wind up costing more, on average, than ones plunked squarely in either New York or New Jersey. What's the raw (uncontrolled) relationship between bid accuracy and location? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ bids$Loc)) 

ggplot(bids, aes(x = Loc, y = accuracy)) + 
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = .5, fun.args = list(mult = 1)) + 
  geom_jitter(size=.8,position=position_jitter(width=.1, height=0), aes(color = Loc)) + 
  theme(legend.position="none") 
```


The signal is stronger regarding the type of project, which has an identifiable (if yet uncontrolled) relationship with estimating accuracy. What's the raw (uncontrolled) relationship between estimation accuracy and the type of project? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$Typeology))) 

ggplot(bids, aes(x = Typeology, y = accuracy)) + 
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = .5, fun.args = list(mult = 1)) + 
  geom_jitter(size=.8,position=position_jitter(width=.1, height=0), aes(color = as.factor(decile))) + 
  theme(legend.position="none") 
```

We see signals that projects of medium size (in dollar terms) may be less evasive than much larger or smaller projects. What's the relationship between low bids and accuracy, when we start considering the size of low bids? 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$decile))) 

ggplot(bids, aes(x = as.factor(decile), y = accuracy)) + 
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = .5, fun.args = list(mult = 1)) + 
  geom_jitter(size=.8,position=position_jitter(width=.1, height=0), aes(color = as.factor(decile))) + 
  theme(legend.position="none") 
```

```{r}
ggplot(bids, aes(x = as.factor(decile), y = accuracy)) + 
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = .5, fun.args = list(mult = 1)) + 
  geom_jitter(size=.8,position=position_jitter(width=.1, height=0), aes(color = as.factor(decile))) + 
  theme(legend.position="none") + 
  facet_wrap( ~ Typeology, ncol=1)
```

Visualizing and testing the data iteratively like this has offered some initial insight into what might be accounting for variation in accuracy. 


# Modeling and prediction

Try and use exogenous covariates to predict an alternative engineering estimate, without using the low bid information, that might be closer to the low bid. 
Call it "expected low bid" or something so we can remember what we're trying to get. 

##A. Base model (manual selection) 
Interpretation: specification was manual and intuitive. 

Note: ensure the "accuracy" variable we calculated earlier is dropped before modeling or I'll be introducing some dual (reverse) causality, which will confuse the models. 

Note 2: when a number appears in the output without context, it is likely an information criterion (and AIC), which may or may not provide value post-modeling. 

First remove accuracy. 
```{r}
train$accuracy=NULL 
test$accuracy=NULL 
```

Run model. 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
base = lm(log(Low.Bid) ~ Engr.Est + Employment.in.construction + Format + Typeology + permits_1 + cci + decile, data = train) 

  summary(base) 
  options(scipen=999)
  mse_b = round(mse(train$Low.Bid,base$fitted.values),0)
  adjr_b = round(summary(base)$adj.r.squared,3)
```

The equation above throws extra information at the engineering estimate and tries to predict the low qualifying bid. If the result is noticably closer to the low qualifying bid than the original estiamte, you can use the delta as a post-estimation fudge factor to adjust your final estimate.

The numbers calculated above include a few metrics to use in comparing the three estimates against the objective data point at the low bid, which is what we're trying to predict. Ghose three estimates I'm talking about are: 
  1. The original, raw engineering cost estimate,
  2. The first alternative, where we built a model by hand to try and use a few more data points to enhance the original estimate, and; 
  3. The second alternative, a kitchen sink model that throws even more data points at the question. This followed an effort to use a penalized regression to identify the best covariates, but that penalization algorithm actually suggested there isn't much we can do to enhance the original estimate. (Note: this will prove prescient.)

I'll build a table near the very end of this script that summarize the metrics I'm using to understand how well these modeling efforts work. The metrics will be: 
  A. A basic t-test to understand whether there's even a statistically significant difference between the estimate I'm getting and the enhanced estimate I'm modeling with it, 
  B. A correlation between the two numbers, to try and understand the magnitude of that difference (if we can trust it really exists), 
  C. Two measures of the predictive modeling power of the models, an adjusted R-squared and the mean squared error (MSE). Both are common metrics of power. The first can be viewed discretely for each model but the second only provides a relative measure between models. 
  
What is the summary of the predicted values? 
How does it compare to the summary of low bids? 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
summary(base$fitted.values) 
summary(train$Low.Bid) 
```

Looking at the two summaries above, what's more accurate, the original engineering estimate or the first enhanced prediction? Neither, really. In fact the estimate and prediction aren't even (statistically) significantly different. Maybe something more robust can come with a little creativity. 

(Note: the model should control to prevent negative values. To be done next time.)

It might have been worth trying with the log of prices, only because the statistical fit becomes multiplicative instead of linear, but that produced similar results. 


## B. Stepwise selection 

Do a backward stepwise selection only because early runs indicate there's little room to add much predictive power, so recognize that the default process backward will favor leaving variables in more than forward. 

COMING SOON. 



## C. Machine learning
Strip the bids data of unusable stuff, then set controls and run.
The package I'm using, glmnet, requires a little extra preparation.

The output below represents the algorithm's effort to look for variatbles that might be dependable in adding predictive power to the original engineering estimate. 

```{r, echo=FALSE, message = FALSE, warning = FALSE}
train2 = train[,-c(1:4,8:10,12,14,52)] 
  train2 = na.omit(train2) 
test2 = test[,-c(1:4,8:10,12,14,52)] 

XP=data.matrix(train2) #XP=data.matrix(train2[,-3]) 
YP=data.matrix(log(train2$Low.Bid))
lasso=cv.glmnet(x=XP, y=YP, alpha=1, nfolds=30, 
                type.measure="mse", standardize=TRUE) #family="gaussian" 

plot(lasso) 
coef(lasso, s=lasso$lambda.1se) 
coef(lasso, s=lasso$lambda.min) 
```

ISL suggests "a large value of s corresponds to [lambda]=0 ... if s is sufficiently large, .... the ... estimates will be the same as the least squares estimates." 

The algorithm suggests using anything beyond the engineer's estimate itself to better predict the lowest qualifying good adds more uncertainty (in the form of noise that's tough to explain) than it adds value. (The "penalty" associated with adding variables is greater than the extra predictive power they bring.) The exception is project size, and that would provide a chance to look closer. 

```{r}
  options(scipen=999)
  estimate_c = lasso$fitted.values #predict(compare,train2) 
  mse_c = round(mse(test$Low.Bid,lasso$fitted.values),0)
  #accuracy_c = cor(test$Low.Bid,lasso$fitted.values) 
  #adjr_c = round(summary(lasso)$adj.r.squared,3)
  #ttest_c = t.test(lasso$fitted.values,test$Engr.Est)$p.value
```


#Accuracy 

Fascinating. 
I first tried calculating a metric that captures accuracy (the relationship between the estimate and the actual). We wanted to use a weighted average of the ratio between estimate and actuals. Something like this ... 

accuracy_a = weighted.mean((train$Engr.Est/train$Low.Bid),train$Low.Bid) 

... applied across all three prisms (uncontrolled and the two models). 

But that calculation just results in 1 (and exactly 1) for both models. 
So we tried something different, summing the estimates and (separately) the actuals, and then dividing those totals. But that also gave me 1 because the average estimate and the average actual are exactly the same, down to the dollar. 

How can we be estimating the value that well, on average? The model must be optimized with respect to the average dependent value. 

But this is a question for another time. For now I'll just need to lean on other metrics as indications of accuracy: 

```{r echo=FALSE}
accuracy_a = cor(train$Engr.Est,train$Low.Bid) 
accuracy_b = cor(base$fitted.values,train$Low.Bid) 
#accuracy_c = cor(lasso$fitted.values,train$Low.Bid) 
```

Well, the correlations between estimates and actuals do show signals of tightening up slightly. But how robust is this, really? Are the numbers really different? And how would the two models perform on fresh data? 

##Compare new estimates to original estimates.
```{r echo=FALSE}
ttest_a = "NA" 
ttest_b = t.test(base$fitted.values,train$Engr.Est)$p.value
#ttest_c = t.test(compare$fitted.values,train$Engr.Est)$p.value
```
No statistically significance differences to be found in either case.

#Validate
##Run on the withheld data and check.

We've already got the models (both of them): "base" and "compare".

```{r, echo=FALSE, message = FALSE, warning = FALSE}
test_accuracy_a = cor(test$Low.Bid,test$Engr.Est) 

base2 = lm(log(Low.Bid) ~ Engr.Est + Employment.in.construction + permits_1 + cci, data = test) 

  options(scipen=999)
  estimate_b2 = base2$fitted.values #predict(compare,train2) 
  test_mse_b = round(mse(test$Low.Bid,base2$fitted.values),0)
  test_accuracy_b = cor(test$Low.Bid,base2$fitted.values) 
  test_adjr_b = round(summary(base2)$adj.r.squared,3)
  test_ttest_b = t.test(base2$fitted.values,test$Engr.Est)$p.value
```

Results for the broader model: 

```{r echo=FALSE}
compare2 = lm(Low.Bid ~ ., data=test2) 
  estimate_c2 = compare2$fitted.values #predict(compare,train2) 
  t.test(estimate_c2,test2$Engr.Est) 
  test_mse_c = round(mse(test2$Low.Bid,estimate_c2),0)
  test_accuracy_c = cor(test2$Low.Bid,compare2$fitted.values) 
  test_adjr_c = round(summary(compare2)$adj.r.squared,3)
  test_ttest_c = t.test(compare2$fitted.values,test2$Engr.Est)$p.value
```


#Compare
```{r echo=FALSE, message = FALSE, warning = FALSE}
#table_1 = c("none","base","lasso") 
#table_2 = c("NA",round(ttest_b,3),round(ttest_c,3)) 
#table_3 = c(round(accuracy_a,3),round(accuracy_b,3),round(accuracy_c,3)) 
#table_4 = c("NA",adjr_b,adjr_c) 
#table_5 = c("NA",formatC(mse_b, big.mark=","),formatC(mse_c, big.mark=",")) 
#table_6 = c("NA",round(test_ttest_b,3),round(test_ttest_c,3))
#table_7 = c(round(test_accuracy_a,3),round(test_accuracy_b,3),round(test_accuracy_c,3))
#table_8 = c("NA",round(test_adjr_b,3),round(test_adjr_c,3))
#table_9 = c("NA",formatC(test_mse_b, big.mark=","),formatC(test_mse_c, big.mark=","))
```

This is a table comparing performance of: 
1. The raw estimates, 
2. The manually-developed model, and, 
3. The informed model 
along some key measures. The p-values (below 0.1 is strong) and correlation (closer to 1 is strong) can be considered discretely for the two models (and, for correlation, the raw estimate), as they reflect how strongly predicted bids and actual bids are related, and the adjusted R-squared (closest to 1 is best) and mean squared error (MSE; lower is best) data points offer technical insight into how well each model performs ... 

For the training exercise (data from 2015-Q1 2019): 
```{r echo=FALSE}
#results_train = as.data.frame(cbind(table_1,table_2,table_3,table_4,
#                      table_5));
#names(results_train) = c("model","p-value","cor","adj-R2","MSE")  
#results_test = as.data.frame(cbind(table_1,table_6,table_7,table_8,table_9));
#names(results_test) = c("model","test_p-value","test_cor","test_adj-R2","test_MSE") 
#rm(table_1,table_2,table_3,table_4,table_5,table_6,table_7,table_8,table_9) 
```

Results: 
```{r}
#t(results_train) 
```

For the test (data withheld, from Q2 and Q3 2019): 
```{r echo=FALSE}
#t(results_test) 
```

# Discussion. 
The model we selected myself doesn't do much better than the engineer's estimate. And the model with automated variable selection via lasso doesn't really do any better than the one by hand. 

The big insight here seems to be the impact project size has on accuracy. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
summary(lm(bids$accuracy ~ as.factor(bids$decile))) 

ggplot(bids, aes(x = as.factor(decile), y = accuracy)) + 
  stat_summary(fun.y = "mean", geom = "col") + 
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = .5, fun.args = list(mult = 1)) + 
  geom_jitter(size=.8,position=position_jitter(width=.1, height=0), aes(color = as.factor(decile))) + 
  theme(legend.position="none")
```
